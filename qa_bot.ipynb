{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1PGHZU3WU79CsEivNJ9gtvY3qHi_2BB0w",
      "authorship_tag": "ABX9TyPDHnb78xJ6Bzef7eJXz/5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilstermanz/holbertonschool-machine_learning/blob/main/supervised_learning/qa_bot/qa_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "RHACq0u66RO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Question Answering\n",
        "\n",
        "Write a function ```def question_answer(question, reference):``` that finds a snippet of text within a reference document to answer a question:\n",
        "\n",
        "    question is a string containing the question to answer\n",
        "    reference is a string containing the reference document from which to find the answer\n",
        "    Returns: a string containing the answer\n",
        "    If no answer is found, return None\n",
        "    Your function should use the bert-uncased-tf2-qa model from the tensorflow-hub library\n",
        "    Your function should use the pre-trained BertTokenizer, bert-large-uncased-whole-word-masking-finetuned-squad, from the transformers library\n"
      ],
      "metadata": {
        "id": "gQEwivSa5XNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "lLLvHSyU8H3Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DadoyQ8k5Ti8"
      },
      "outputs": [],
      "source": [
        "model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def question_answer(question, reference):\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    reference_tokens = tokenizer.tokenize(reference)\n",
        "    tokens = ['[CLS]'] + question_tokens + ['[SEP]']\\\n",
        "        + reference_tokens + ['[SEP]']\n",
        "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_word_ids)\n",
        "    input_type_ids = [0] * (1 + len(\n",
        "        question_tokens) + 1) + [1] * (len(reference_tokens) + 1)\n",
        "\n",
        "    input_word_ids, input_mask, input_type_ids = map(\n",
        "        lambda t: tf.expand_dims(tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
        "        (input_word_ids, input_mask, input_type_ids))\n",
        "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "    answer_tokens = tokens[short_start: short_end + 1]\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        answer_tokens) if len(answer_tokens) > 0 else None\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "oJbKnMY8FmlA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "print(question_answer('When are PLDs?', reference))\n",
        "print(question_answer('What does PLD stand for?', reference))\n",
        "print(question_answer('What are Mock Interviews?', reference))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxDEDJbi97s6",
        "outputId": "389c912e-942d-4440-b4d7-99f010df7bcd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on - site days from 9 : 00 am to 3 : 00 pm\n",
            "peer learning days\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Create the loop\n",
        "\n",
        "Create a script that takes in input from the user with the prompt ```Q:``` and prints ```A:``` as a response. If the user inputs ```exit```, ```quit```, ```goodbye```, or ```bye```, case insensitive, print ```A: Goodbye``` and exit."
      ],
      "metadata": {
        "id": "hEMjGw7BG9mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cmd"
      ],
      "metadata": {
        "id": "eq84pi5sGrbe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loop():\n",
        "    exit_cmds = ['exit', 'quit', 'goodbye', 'bye']\n",
        "    prompt = 'Q: '\n",
        "\n",
        "    # Prompt for question\n",
        "    q = input(prompt).lower()\n",
        "\n",
        "    while q not in exit_cmds:\n",
        "\n",
        "        # Find answer\n",
        "        answer = ''\n",
        "\n",
        "        # Print answer\n",
        "        print('A:', answer)\n",
        "\n",
        "        # Prompt for next question\n",
        "        q = input(prompt).lower()\n",
        "\n",
        "    # End loop\n",
        "    print('A: Goodbye')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHG-HfhJIi0l",
        "outputId": "ad4e275a-c725-4a35-b3a1-90fb6740f591"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Hello\n",
            "A: \n",
            "Q: How are you?\n",
            "A: \n",
            "Q: BYE\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Answer Questions\n",
        "\n",
        "Based on the previous tasks, write a function ```def answer_loop(reference):``` that answers questions from a reference text:\n",
        "\n",
        "    reference is the reference text\n",
        "    If the answer cannot be found in the reference text, respond with Sorry, I do not understand your question.\n"
      ],
      "metadata": {
        "id": "7oT2156POQ3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_loop(reference):\n",
        "    exit_cmds = ['exit', 'quit', 'goodbye', 'bye']\n",
        "    prompt = 'Q: '\n",
        "    reference_tokens = tokenizer.tokenize(reference)\n",
        "\n",
        "    # Prompt for question\n",
        "    q = input(prompt).lower()\n",
        "\n",
        "    while q not in exit_cmds:\n",
        "\n",
        "        # Find answer\n",
        "        question_tokens = tokenizer.tokenize(q)\n",
        "        tokens = ['[CLS]'] + question_tokens + ['[SEP]']\\\n",
        "            + reference_tokens + ['[SEP]']\n",
        "        input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(input_word_ids)\n",
        "        input_type_ids = [0] * (1 + len(\n",
        "            question_tokens) + 1) + [1] * (len(reference_tokens) + 1)\n",
        "\n",
        "        input_word_ids, input_mask, input_type_ids = map(\n",
        "            lambda t: tf.expand_dims(\n",
        "                tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
        "            (input_word_ids, input_mask, input_type_ids))\n",
        "        outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "        short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "        short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "        answer_tokens = tokens[short_start: short_end + 1]\n",
        "\n",
        "        if len(answer_tokens) > 0:\n",
        "            answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "        else:\n",
        "            answer = 'Sorry, I do not understand your question.'\n",
        "\n",
        "        # Print answer\n",
        "        print('A:', answer)\n",
        "\n",
        "        # Prompt for next question\n",
        "        q = input(prompt).lower()\n",
        "\n",
        "    # End loop\n",
        "    print('A: Goodbye')"
      ],
      "metadata": {
        "id": "tHy8mzlpNBNI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "answer_loop(reference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lO8T1VoRnUB",
        "outputId": "86d28bf6-1f45-43a5-93db-7c103e1780dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: When are PLDs\n",
            "A: mandatory on - site days from 9 : 00 am to 3 : 00 pm\n",
            "Q: What does PLD stand for?\n",
            "A: peer learning days\n",
            "Q: What are mock interviews?\n",
            "A: Sorry, I do not understand your question.\n",
            "Q: GOoDBye\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Semantic Search\n",
        "\n",
        "Write a function def semantic_search(corpus_path, sentence): that performs semantic search on a corpus of documents:\n",
        "\n",
        "    corpus_path is the path to the corpus of reference documents on which to perform semantic search\n",
        "    sentence is the sentence from which to perform semantic search\n",
        "    Returns: the reference text of the document most similar to sentence\n"
      ],
      "metadata": {
        "id": "nIo4vcYVToSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "cN6LW7k5XAc8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ],
      "metadata": {
        "id": "QxInuXS2d9IN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(corpus_path, sentence):\n",
        "    corpus = [sentence]\n",
        "    for filename in glob.glob(corpus_path + '/*'):\n",
        "        with open(filename, 'r') as file:\n",
        "            corpus.append(file.read())\n",
        "\n",
        "    embeddings = np.array(embed(corpus))\n",
        "\n",
        "    cos_sims = []\n",
        "    for embedding in embeddings:\n",
        "        cos_sims.append(cosine_similarity([embeddings[0]], [embedding]))\n",
        "    max_index = np.array(cos_sims[1:]).argmax() + 1\n",
        "\n",
        "    return corpus[max_index]"
      ],
      "metadata": {
        "id": "LhXq1GF2sCWy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(semantic_search('/content/drive/MyDrive/ZendeskArticles/', 'When are PLDs?'))\n",
        "print(semantic_search('/content/drive/MyDrive/ZendeskArticles/', 'What is a Mock Interview'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXub1m1RWYP-",
        "outputId": "57b39fdc-72df-4340-8c59-7fa61e9dfba4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PLD Overview\n",
            "Peer Learning Days (PLDs) are a time for you and your peers to ensure that each of you understands the concepts you've encountered in your projects, as well as a time for everyone to collectively grow in technical, professional, and soft skills. During PLD, you will collaboratively review prior projects with a group of cohort peers.\n",
            "PLD Basics\n",
            "PLDs are mandatory on-site days from 9:00 AM to 3:00 PM. If you cannot be present or on time, you must use a PTO. \n",
            "No laptops, tablets, or screens are allowed until all tasks have been whiteboarded and understood by the entirety of your group. This time is for whiteboarding, dialogue, and active peer collaboration. After this, you may return to computers with each other to pair or group program. \n",
            "Peer Learning Days are not about sharing solutions. This doesn't empower peers with the ability to solve problems themselves! Peer learning is when you share your thought process, whether through conversation, whiteboarding, debugging, or live coding. \n",
            "When a peer has a question, rather than offering the solution, ask the following:\n",
            "\"How did you come to that conclusion?\"\n",
            "\"What have you tried?\"\n",
            "\"Did the man page give you a lead?\"\n",
            "\"Did you think about this concept?\"\n",
            "Modeling this form of thinking for one another is invaluable and will strengthen your entire cohort.\n",
            "Your ability to articulate your knowledge is a crucial skill and will be required to succeed during technical interviews and through your career. \n",
            "\n",
            "Mock Interview Help\n",
            "The interviewer is allowed to discuss and ask questions in order to help the candidate find the correct solution or to get more information when needed.\n",
            "You can help the candidate on both the technical conversation and the code.\n",
            "Subtract 0.1 point each time you helped the candidate find the correct or complete solution by asking questions. \n",
            "If you feel you are helping the candidate too much, guide them to the right answer as much as possible or give the answer in order to move the mock interview forward.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Multi-reference Question Answering\n",
        "\n",
        "Based on the previous tasks, write a function ```def question_answer(corpus_path):``` that answers questions from multiple reference texts:\n",
        "\n",
        "    corpus_path is the path to the corpus of reference documents\n"
      ],
      "metadata": {
        "id": "afMDxKOHv08J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(corpus_path, sentence):\n",
        "    corpus = [sentence]\n",
        "    for filename in glob.glob(corpus_path + '/*'):\n",
        "        with open(filename, 'r') as file:\n",
        "            corpus.append(file.read())\n",
        "\n",
        "    embeddings = np.array(embed(corpus))\n",
        "\n",
        "    cos_sims = []\n",
        "    for embedding in embeddings:\n",
        "        cos_sims.append(cosine_similarity([embeddings[0]], [embedding]))\n",
        "    max_index = np.array(cos_sims[1:]).argmax() + 1\n",
        "\n",
        "    return corpus[max_index]\n",
        "\n",
        "def question_answer(corpus_path):\n",
        "    exit_cmds = ['exit', 'quit', 'goodbye', 'bye']\n",
        "    prompt = 'Q: '\n",
        "\n",
        "    # Prompt for question\n",
        "    q = input(prompt).lower()\n",
        "\n",
        "    while q not in exit_cmds:\n",
        "\n",
        "        # Find reference\n",
        "        reference = semantic_search(corpus_path, q)\n",
        "\n",
        "        # Find answer\n",
        "        question_tokens = tokenizer.tokenize(q)\n",
        "        reference_tokens = tokenizer.tokenize(reference)\n",
        "        tokens = ['[CLS]'] + question_tokens + ['[SEP]']\\\n",
        "            + reference_tokens + ['[SEP]']\n",
        "        input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(input_word_ids)\n",
        "        input_type_ids = [0] * (1 + len(\n",
        "            question_tokens) + 1) + [1] * (len(reference_tokens) + 1)\n",
        "\n",
        "        input_word_ids, input_mask, input_type_ids = map(\n",
        "            lambda t: tf.expand_dims(\n",
        "                tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
        "            (input_word_ids, input_mask, input_type_ids))\n",
        "        outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "        short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "        short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "        answer_tokens = tokens[short_start: short_end + 1]\n",
        "\n",
        "        if len(answer_tokens) > 0:\n",
        "            answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "        else:\n",
        "            answer = 'Sorry, I do not understand your question.'\n",
        "\n",
        "        # Print answer\n",
        "        print('A:', answer)\n",
        "\n",
        "        # Prompt for next question\n",
        "        q = input(prompt).lower()\n",
        "\n",
        "    # End loop\n",
        "    print('A: Goodbye')"
      ],
      "metadata": {
        "id": "2oQP19MpveJP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer('/content/drive/MyDrive/ZendeskArticles/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nobghrs9w0FT",
        "outputId": "a3a30f97-a9b6-4b61-f756-d867ce5b3022"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: When are PLDs?\n",
            "A: on - site days from 9 : 00 am to 3 : 00 pm\n",
            "Q: What are Mock Interviews?\n",
            "A: help you train for technical interviews\n",
            "Q: What does PLD stand for?\n",
            "A: peer learning days\n",
            "Q: What is Holberton?\n",
            "A: a supportive learning space with clear expectations\n",
            "Q: What is the framework?\n",
            "A: building foundations\n",
            "Q: answer this\n",
            "A: Sorry, I do not understand your question.\n",
            "Q: bye\n",
            "A: Goodbye\n"
          ]
        }
      ]
    }
  ]
}